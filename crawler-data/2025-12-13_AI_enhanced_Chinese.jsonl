{"id": "2512.10865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10865", "abs": "https://arxiv.org/abs/2512.10865", "authors": ["Lilin Qiu"], "title": "Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python", "comment": null, "summary": "This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.", "AI": {"tldr": "\u4f7f\u7528\u8ba1\u7b97\u6587\u672c\u5206\u6790\u65b9\u6cd5\u7814\u7a76\u300a\u970d\u6bd4\u7279\u4eba\u300b\u5bf9\u8bdd\u7684\u60c5\u611f\u57fa\u8c03\uff0c\u53d1\u73b0\u5bf9\u8bdd\u4fdd\u6301\u79ef\u6781\u3001\u5e73\u9759\u7684\u57fa\u8c03\uff0c\u968f\u7740\u6545\u4e8b\u53d1\u5c55\u89d2\u8272\u80fd\u52a8\u6027\u9010\u6e10\u589e\u5f3a\uff0c\u4f53\u73b0\u4e86\u5c0f\u8bf4\u4e2d\u7d27\u5f20\u4e0e\u8212\u9002\u4ea4\u66ff\u7684\u60c5\u611f\u8282\u594f\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8ba1\u7b97\u6587\u672c\u5206\u6790\u65b9\u6cd5\u63ed\u793a\u6587\u5b66\u4f5c\u54c1\u4e2d\u7684\u5fae\u5999\u60c5\u611f\u7ed3\u6784\uff0c\u7279\u522b\u662f\u7814\u7a76\u6258\u5c14\u91d1\u300a\u970d\u6bd4\u7279\u4eba\u300b\u4e2d\u5bf9\u8bdd\u7684\u60c5\u611f\u57fa\u8c03\u5982\u4f55\u5851\u9020\u6545\u4e8b\u8bb2\u8ff0\u3002", "method": "\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u63d0\u53d6\u5bf9\u8bdd\u6587\u672c\uff0c\u8fdb\u884c\u9884\u5904\u7406\u540e\uff0c\u91c7\u7528NRC-VAD\u8bcd\u5178\u5bf9\u60c5\u611f\u7ef4\u5ea6\u8fdb\u884c\u91cf\u5316\u8bc4\u5206\uff0c\u5206\u6790\u60c5\u611f\u8f68\u8ff9\u5e76\u751f\u6210\u53ef\u89c6\u5316\u56fe\u8868\u3002", "result": "\u5bf9\u8bdd\u6574\u4f53\u4fdd\u6301\u79ef\u6781\uff08\u9ad8\u60c5\u611f\u4ef7\uff09\u548c\u5e73\u9759\uff08\u4f4e\u5524\u9192\u5ea6\uff09\u7684\u57fa\u8c03\uff0c\u968f\u7740\u6545\u4e8b\u8fdb\u5c55\u89d2\u8272\u80fd\u52a8\u6027\uff08\u652f\u914d\u5ea6\uff09\u9010\u6e10\u589e\u5f3a\uff0c\u60c5\u611f\u8f68\u8ff9\u53ef\u89c6\u5316\u663e\u793a\u6258\u5c14\u91d1\u7684\u8bed\u8a00\u5728\u7d27\u5f20\u4e0e\u8212\u9002\u4e4b\u95f4\u5faa\u73af\u3002", "conclusion": "\u8ba1\u7b97\u5de5\u5177\u4e0e\u6587\u5b66\u89e3\u8bfb\u76f8\u7ed3\u5408\u53ef\u4ee5\u63ed\u793a\u6587\u5b66\u4f5c\u54c1\u4e2d\u7684\u5fae\u5999\u60c5\u611f\u7ed3\u6784\uff0c\u5c55\u73b0\u4e86\u300a\u970d\u6bd4\u7279\u4eba\u300b\u4e2d\u5851\u9020\u6545\u4e8b\u8bb2\u8ff0\u7684\u7a33\u5b9a\u8282\u594f\u548c\u60c5\u611f\u8c03\u8282\u673a\u5236\u3002"}}
{"id": "2512.10882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10882", "abs": "https://arxiv.org/abs/2512.10882", "authors": ["Hauke Licht"], "title": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity", "comment": null, "summary": "Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u60c5\u7eea\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u6a21\u578b\u8868\u73b0\u826f\u597d\u4e14\u65e0\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\uff0c\u4f46\u5728\u771f\u5b9e\u8bae\u4f1a\u8fa9\u8bba\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u5f71\u54cd\u4e0b\u6e38\u7edf\u8ba1\u63a8\u65ad\u3002", "motivation": "\u60c5\u7eea\u5728\u653f\u6cbb\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u968f\u7740\u7814\u7a76\u8d8a\u6765\u8d8a\u591a\u5730\u5229\u7528\u89c6\u542c\u6750\u6599\u5206\u6790\u60c5\u7eea\u8868\u8fbe\uff0c\u591a\u6a21\u6001\u751f\u6210AI\u7684\u51fa\u73b0\u5e26\u6765\u4e86\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u591a\u6a21\u6001AI\u5728\u60c5\u7eea\u5206\u6790\u4e2d\u6709\u6548\u6027\u7684\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u60c5\u7eea\u5524\u9192\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u4e24\u4e2a\u4e92\u8865\u7684\u4eba\u7c7b\u6807\u8bb0\u89c6\u9891\u6570\u636e\u96c6\uff1a\u7406\u60f3\u6761\u4ef6\u4e0b\u7684\u8bb0\u5f55\u548c\u771f\u5b9e\u8bae\u4f1a\u8fa9\u8bba\u4e2d\u7684\u6f14\u8bb2\u8005\u8bb0\u5f55\u3002", "result": "\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\uff0cmLLMs\u7684\u60c5\u7eea\u5524\u9192\u8bc4\u5206\u9ad8\u5ea6\u53ef\u9760\uff0c\u51e0\u4e4e\u6ca1\u6709\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\u8ff9\u8c61\u3002\u4f46\u5728\u771f\u5b9e\u8bae\u4f1a\u8fa9\u8bba\u8bb0\u5f55\u4e2d\uff0cmLLMs\u7684\u5524\u9192\u8bc4\u5206\u672a\u80fd\u5151\u73b0\u5176\u6f5c\u529b\uff0c\u53ef\u80fd\u5bf9\u4e0b\u6e38\u7edf\u8ba1\u63a8\u65ad\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u653f\u6cbb\u5206\u6790\u4e2d\u6301\u7eed\u3001\u5f7b\u5e95\u8bc4\u4f30\u65b0\u5174\u751f\u6210AI\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u5236\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2512.10949", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10949", "abs": "https://arxiv.org/abs/2512.10949", "authors": ["Yiwen Tang", "Zoey Guo", "Kaixin Zhu", "Ray Zhang", "Qizhi Chen", "Dongzhi Jiang", "Junli Liu", "Bohan Zeng", "Haoming Song", "Delin Qu", "Tianyi Bai", "Dan Xu", "Wentao Zhang", "Bin Zhao"], "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "comment": "Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1", "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u6587\u672c\u52303D\u81ea\u56de\u5f52\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86AR3D-R1\u6a21\u578b\uff0c\u901a\u8fc7\u5956\u52b1\u8bbe\u8ba1\u3001\u7b97\u6cd5\u4f18\u5316\u548c\u5206\u5c42\u751f\u6210\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863D\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u57282D\u56fe\u50cf\u751f\u6210\u4e2d\u5df2\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u7531\u4e8e3D\u5bf9\u8c61\u5177\u6709\u66f4\u9ad8\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u548c\u4e00\u81f4\u6027\u8981\u6c42\uff0c\u5c06\u5176\u5e94\u7528\u4e8e3D\u751f\u6210\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9RL\u57283D\u751f\u6210\u4e2d\u7cfb\u7edf\u6027\u7684\u63a2\u7d22\u3002", "method": "1) \u7cfb\u7edf\u8bc4\u4f30\u5956\u52b1\u8bbe\u8ba1\u548c\u6a21\u578b\u9009\u62e9\uff1b2) \u7814\u7a76GRPO\u53d8\u4f53\u7b97\u6cd5\uff0c\u5f3a\u8c03\u8bcd\u5143\u7ea7\u4f18\u5316\u7684\u6709\u6548\u6027\uff1b3) \u5f15\u5165MME-3DR\u57fa\u51c6\u6d4b\u8bd5\uff1b4) \u63d0\u51faHi-GRPO\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u4e13\u7528\u5956\u52b1\u96c6\u6210\u4f18\u5316\u5168\u5c40\u5230\u5c40\u90e8\u76843D\u751f\u6210\u3002", "result": "\u5f00\u53d1\u4e86AR3D-R1\u6a21\u578b\uff0c\u8fd9\u662f\u9996\u4e2aRL\u589e\u5f3a\u7684\u6587\u672c\u52303D\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u7c97\u7c92\u5ea6\u5f62\u72b6\u5230\u7eb9\u7406\u7ec6\u5316\u8fdb\u884c\u4e13\u4e1a\u7ea7\u751f\u6210\u3002\u7814\u7a76\u8868\u660e\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u5956\u52b1\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u591a\u6a21\u6001\u6a21\u578b\u80fd\u4e3a3D\u5c5e\u6027\u63d0\u4f9b\u7a33\u5065\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aRL\u9a71\u52a8\u76843D\u751f\u6210\u63a8\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u53473D\u751f\u6210\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u901a\u8fc7\u5206\u5c42\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u5904\u74063D\u5bf9\u8c61\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7ec6\u8282\u3002"}}
{"id": "2512.10895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10895", "abs": "https://arxiv.org/abs/2512.10895", "authors": ["Lijie Ding", "Janell Thomson", "Jon Taylor", "Changwoo Do"], "title": "LLMs Can Assist with Proposal Selection at Large User Facilities", "comment": "9 pages, 8figures", "summary": "We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $\u03c1\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.", "AI": {"tldr": "LLMs\u53ef\u7528\u4e8e\u5927\u578b\u7528\u6237\u8bbe\u65bd\u7684\u63d0\u6848\u9009\u62e9\uff0c\u63d0\u4f9b\u6bd4\u4f20\u7edf\u4eba\u5de5\u8bc4\u5ba1\u66f4\u53ef\u6269\u5c55\u3001\u4e00\u81f4\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u6392\u540d\u548c\u8bc6\u522b\u9ad8\u53d1\u8868\u6f5c\u529b\u63d0\u6848\u65b9\u9762\u8868\u73b0\u4e0e\u4eba\u7c7b\u76f8\u5f53\uff0c\u6210\u672c\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u63d0\u6848\u8bc4\u5ba1\u5b58\u5728\u63d0\u6848\u95f4\u76f8\u5173\u6027\u5f31\u3001\u8bc4\u5ba1\u8005\u504f\u89c1\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u6210\u5bf9\u504f\u597d\u7684\u65b9\u6cd5\u867d\u7136\u903b\u8f91\u4e0a\u66f4\u4f18\u8d8a\uff0c\u4f46\u4e8c\u6b21\u65b9\u5de5\u4f5c\u91cf\u4f7f\u5176\u5bf9\u4eba\u7c7b\u8bc4\u5ba1\u8005\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u5229\u7528LLMs\u8fdb\u884c\u63d0\u6848\u9009\u62e9\uff0c\u57fa\u4e8e\u7f8e\u56fd\u6a61\u6811\u5cad\u56fd\u5bb6\u5b9e\u9a8c\u5ba4\u6563\u88c2\u4e2d\u5b50\u6e90\u4e09\u4e2a\u675f\u7ebf\u7684\u7cbe\u5fc3\u7b56\u5212\u7684\u63d0\u6848\u548c\u53d1\u8868\u8bb0\u5f55\uff0c\u91c7\u7528\u6210\u5bf9\u504f\u597d\u65b9\u6cd5\u8fdb\u884c\u6392\u540d\uff0c\u5e76\u4f7f\u7528\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u63d0\u6848\u76f8\u4f3c\u6027\u5b9a\u91cf\u8bc4\u4f30\u3002", "result": "LLM\u6392\u540d\u4e0e\u4eba\u7c7b\u6392\u540d\u5f3a\u76f8\u5173\uff08Spearman \u03c1\u22480.2-0.8\uff0c\u53bb\u966410%\u5f02\u5e38\u503c\u540e\u22650.5\uff09\uff0c\u5728\u8bc6\u522b\u9ad8\u53d1\u8868\u6f5c\u529b\u63d0\u6848\u65b9\u9762\u8868\u73b0\u4e0d\u5dee\u4e8e\u4eba\u7c7b\u8bc4\u5ba1\u8005\uff0c\u6210\u672c\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002", "conclusion": "LLMs\u4e3a\u5927\u578b\u7528\u6237\u8bbe\u65bd\u7684\u63d0\u6848\u9009\u62e9\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4e00\u81f4\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u80fd\u8fdb\u884c\u6709\u6548\u6392\u540d\uff0c\u8fd8\u80fd\u5b9e\u73b0\u4eba\u7c7b\u96be\u4ee5\u5b8c\u6210\u7684\u63d0\u6848\u76f8\u4f3c\u6027\u5206\u6790\u7b49\u9ad8\u7ea7\u5206\u6790\u3002"}}
{"id": "2512.10903", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10903", "abs": "https://arxiv.org/abs/2512.10903", "authors": ["Muhammad Umair Haider", "Hammad Rizwan", "Hassan Sajjad", "A. B. Siddique"], "title": "Multi-Granular Node Pruning for Circuit Discovery", "comment": null, "summary": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u8def\u53d1\u73b0\u7684\u8282\u70b9\u7ea7\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u53ef\u5b66\u4e60\u63a9\u7801\u548c\u7c92\u5ea6\u7279\u5b9a\u7a00\u758f\u60e9\u7f5a\uff0c\u5728\u5355\u6b21\u5fae\u8c03\u4e2d\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u7535\u8def\u8bc6\u522b\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u73b0\u6709\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8fed\u4ee3\u8fb9\u526a\u679d\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4ec5\u9650\u4e8e\u7c97\u7c92\u5ea6\u5355\u5143\uff08\u5982\u6ce8\u610f\u529b\u5934\u6216MLP\u5757\uff09\uff0c\u5ffd\u7565\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u7ed3\u6784\u5982\u5355\u4e2a\u795e\u7ecf\u5143\u3002", "method": "\u63d0\u51fa\u8282\u70b9\u7ea7\u526a\u679d\u6846\u67b6\uff0c\u5f15\u5165\u8de8\u591a\u4e2a\u7c92\u5ea6\uff08\u4ece\u6574\u4e2a\u5757\u5230\u5355\u4e2a\u795e\u7ecf\u5143\uff09\u7684\u53ef\u5b66\u4e60\u63a9\u7801\uff0c\u5728\u7edf\u4e00\u4f18\u5316\u76ee\u6807\u4e2d\u4f7f\u7528\u7c92\u5ea6\u7279\u5b9a\u7a00\u758f\u60e9\u7f5a\u6307\u5bfc\u526a\u679d\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u53d1\u73b0\u7684\u7535\u8def\u8282\u70b9\u6570\u5c11\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u8bc1\u660e\u8bb8\u591a\u88ab\u7c97\u7c92\u5ea6\u65b9\u6cd5\u8ba4\u4e3a\u91cd\u8981\u7684\u795e\u7ecf\u5143\u5b9e\u9645\u4e0a\u65e0\u5173\u7d27\u8981\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u5185\u5b58\u5360\u7528\u964d\u4f4e5-10\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u8282\u70b9\u7ea7\u526a\u679d\u6846\u67b6\u89e3\u51b3\u4e86\u7535\u8def\u53d1\u73b0\u7684\u53ef\u6269\u5c55\u6027\u548c\u7c92\u5ea6\u9650\u5236\u95ee\u9898\uff0c\u80fd\u591f\u8bc6\u522b\u66f4\u7ec6\u7c92\u5ea6\u7684\u7535\u8def\u7ed3\u6784\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2512.10937", "categories": ["cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.10937", "abs": "https://arxiv.org/abs/2512.10937", "authors": ["Matt Wilson"], "title": "On Decision-Making Agents and Higher-Order Causal Processes", "comment": null, "summary": "We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u4e2d\u7684\u667a\u80fd\u4f53\u4e0e\u5355\u8f93\u5165\u8fc7\u7a0b\u51fd\u6570\u4e4b\u95f4\u7684\u7cbe\u786e\u5bf9\u5e94\u5173\u7cfb\uff0c\u63ed\u793a\u4e86AI\u51b3\u7b56\u4e0e\u91cf\u5b50\u64cd\u4f5c\u7ecf\u5178\u6781\u9650\u4e4b\u95f4\u7684\u6df1\u5c42\u8054\u7cfb\u3002", "motivation": "\u63a2\u7d22AI\u51b3\u7b56\u7406\u8bba\u4e0e\u91cf\u5b50\u4fe1\u606f\u7406\u8bba\u4e4b\u95f4\u7684\u4ea4\u53c9\u8054\u7cfb\uff0c\u4e3a\u7406\u89e3\u667a\u80fd\u4f53\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u65b0\u7684\u6570\u5b66\u6846\u67b6\u548c\u7269\u7406\u89c6\u89d2\u3002", "method": "\u901a\u8fc7\u5c06POMDP\u667a\u80fd\u4f53\u7684\u7b56\u7565\u548c\u8bb0\u5fc6\u66f4\u65b0\u7ed3\u5408\u6210\u8fc7\u7a0b\u51fd\u6570w\uff0c\u4f7f\u7528\u94fe\u63a5\u79ef\u4e0ePOMDP\u73af\u5883\u4ea4\u4e92\uff0c\u5efa\u7acb\u5f62\u5f0f\u5316\u5bf9\u5e94\u5173\u7cfb\u3002\u5c06\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8bc6\u522b\u89c2\u6d4b\u72ec\u7acb\u5206\u6563POMDP\u4f5c\u4e3a\u591a\u8f93\u5165\u8fc7\u7a0b\u51fd\u6570\u7684\u81ea\u7136\u9886\u57df\u3002", "result": "\u5efa\u7acb\u4e86POMDP\u667a\u80fd\u4f53\u4e0e\u5355\u8f93\u5165\u8fc7\u7a0b\u51fd\u6570\u4e4b\u95f4\u7684\u7cbe\u786e\u5bf9\u5e94\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u53cc\u91cd\u89e3\u91ca\uff1a\u7269\u7406\u89c6\u89d2\u4e2d\u8fc7\u7a0b\u51fd\u6570\u4f5c\u4e3a\u73af\u5883\uff0cAI\u89c6\u89d2\u4e2d\u8fc7\u7a0b\u51fd\u6570\u7f16\u7801\u667a\u80fd\u4f53\u3002\u6210\u529f\u5c06\u6846\u67b6\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aAI\u51b3\u7b56\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u5b66\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u51b3\u7b56\u4e0e\u91cf\u5b50\u64cd\u4f5c\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.10863", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10863", "abs": "https://arxiv.org/abs/2512.10863", "authors": ["Jingli Lin", "Runsen Xu", "Shaohao Zhu", "Sihan Yang", "Peizhou Cao", "Yunlong Ran", "Miao Hu", "Chenming Zhu", "Yiman Xie", "Yilin Long", "Wenbo Hu", "Dahua Lin", "Tai Wang", "Jiangmiao Pang"], "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence", "comment": null, "summary": "Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.", "AI": {"tldr": "MMSI-Video-Bench\u662f\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u9891\u7a7a\u95f4\u667a\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,106\u4e2a\u95ee\u9898\uff0c\u8986\u76d6\u611f\u77e5\u3001\u89c4\u5212\u3001\u9884\u6d4b\u548c\u8de8\u89c6\u9891\u63a8\u7406\u56db\u4e2a\u5c42\u6b21\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30MLLMs\u5728\u8fde\u7eed\u89c6\u89c9\u8f93\u5165\u4e2d\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u800c\u8fd9\u662fMLLMs\u6210\u4e3a\u7269\u7406\u73af\u5883\u4e2d\u901a\u7528\u52a9\u624b\u7684\u5173\u952e\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u5168\u4eba\u5de5\u6807\u6ce8\u7684\u89c6\u9891\u7a7a\u95f4\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u56db\u5c42\u6846\u67b6\uff08\u611f\u77e5\u3001\u89c4\u5212\u3001\u9884\u6d4b\u3001\u8de8\u89c6\u9891\u63a8\u7406\uff09\uff0c\u5305\u542b1,106\u4e2a\u95ee\u9898\uff0c\u57fa\u4e8e25\u4e2a\u6570\u636e\u96c6\u548c\u5185\u90e8\u89c6\u9891\u76841,278\u4e2a\u7247\u6bb5\uff0c\u6bcf\u4e2a\u95ee\u9898\u75313DV\u4e13\u5bb6\u7cbe\u5fc3\u8bbe\u8ba1\u548c\u5ba1\u67e5\u3002", "result": "\u8bc4\u4f30\u4e8625\u4e2a\u5f00\u6e90\u548c\u4e13\u6709MLLMs\uff0c\u53d1\u73b0\u663e\u8457\u7684\u4eba\u673a\u5dee\u8ddd\uff1a\u8bb8\u591a\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u6700\u4f73\u63a8\u7406\u6a21\u578b\u843d\u540e\u4eba\u7c7b\u8fd160%\u3002\u7a7a\u95f4\u5fae\u8c03\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u4e0d\u8db3\uff0c\u5b58\u5728\u51e0\u4f55\u63a8\u7406\u3001\u8fd0\u52a8\u57fa\u7840\u3001\u957f\u65f6\u9884\u6d4b\u548c\u8de8\u89c6\u9891\u5bf9\u5e94\u7b49\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "conclusion": "MMSI-Video-Bench\u4e3a\u63a8\u8fdb\u89c6\u9891\u7a7a\u95f4\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u7684\u4e25\u91cd\u4e0d\u8db3\uff0c\u5e76\u8868\u660e\u5178\u578b\u5e27\u91c7\u6837\u7b56\u7565\u30013D\u7a7a\u95f4\u7ebf\u7d22\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u5f53\u524d\u57fa\u51c6\u4e0a\u6548\u679c\u6709\u9650\u3002"}}
{"id": "2512.10932", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10932", "abs": "https://arxiv.org/abs/2512.10932", "authors": ["Shengao Wang", "Wenqi Wang", "Zecheng Wang", "Max Whitton", "Michael Wakeham", "Arjun Chandra", "Joey Huang", "Pengyue Zhu", "Helen Chen", "David Li", "Jeffrey Li", "Shawn Li", "Andrew Zagula", "Amy Zhao", "Andrew Zhu", "Sayaka Nakamura", "Yuki Yamamoto", "Jerry Jun Yokono", "Aaron Mueller", "Bryan A. Plummer", "Kate Saenko", "Venkatesh Saligrama", "Boqing Gong"], "title": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models", "comment": null, "summary": "Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.", "AI": {"tldr": "BabyVLM-V2\u662f\u4e00\u4e2a\u57fa\u4e8e\u513f\u7ae5\u53d1\u5c55\u8f68\u8ff9\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7eb5\u5411\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u8ba4\u77e5\u8bc4\u4f30\u5de5\u5177\u7bb1\uff0c\u5b9e\u73b0\u4e86\u4ece\u5c0f\u89c4\u6a21\u6570\u636e\u4e2d\u9ad8\u6548\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u65e9\u671f\u513f\u7ae5\u53d1\u5c55\u8f68\u8ff9\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u6548\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u81ea\u7136\u76ee\u6807\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5efa\u7acb\u4e00\u4e2a\u53d1\u5c55\u5fc3\u7406\u5b66\u57fa\u7840\u624e\u5b9e\u7684\u6846\u67b6\uff0c\u6a21\u62df\u5a74\u513f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4ece\u5c0f\u89c4\u6a21\u6570\u636e\u4e2d\u9ad8\u6548\u5b66\u4e60\u3002", "method": "1) \u6784\u5efa\u7eb5\u5411\u3001\u591a\u65b9\u9762\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u6700\u5927\u5316\u8986\u76d6\u5a74\u513f\u4e2d\u5fc3\u7684\u591a\u5a92\u4f53\u8bed\u6599\uff0c\u5305\u62ec\u89c6\u9891-\u8bdd\u8bed\u3001\u56fe\u50cf-\u8bdd\u8bed\u548c\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\uff1b2) \u5f00\u53d1DevCV\u5de5\u5177\u7bb1\uff0c\u5c06NIH\u5a74\u513f\u5de5\u5177\u7bb1\u4e2d\u7684\u6240\u6709\u89c6\u89c9\u76f8\u5173\u6d4b\u91cf\u6307\u6807\u8f6c\u5316\u4e3a\u5305\u542b10\u4e2a\u591a\u6a21\u6001\u4efb\u52a1\u7684\u57fa\u51c6\u5957\u4ef6\uff0c\u6db5\u76d6\u7a7a\u95f4\u63a8\u7406\u3001\u8bb0\u5fc6\u548c\u8bcd\u6c47\u7406\u89e3\uff1b3) \u4ece\u5934\u5f00\u59cb\u9884\u8bad\u7ec3\u7d27\u51d1\u6a21\u578b\u3002", "result": "\u7d27\u51d1\u6a21\u578b\u5728DevCV\u5de5\u5177\u7bb1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u751a\u81f3\u8d85\u8d8a\u4e86GPT-4o\u7684\u6027\u80fd\u3002\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u4ece\u5c0f\u89c4\u6a21\u6570\u636e\u4e2d\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002", "conclusion": "BabyVLM-V2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u3001\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u6709\u671b\u52a0\u901f\u57fa\u4e8e\u53d1\u5c55\u5fc3\u7406\u5b66\u539f\u7406\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u7814\u7a76\uff0c\u5b9e\u73b0\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u53d1\u5c55\u8f68\u8ff9\u7684\u4eba\u5de5\u667a\u80fd\u5b66\u4e60\u3002"}}
{"id": "2512.10935", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.10935", "abs": "https://arxiv.org/abs/2512.10935", "authors": ["Jay Karhade", "Nikhil Keetha", "Yuchen Zhang", "Tanisha Gupta", "Akash Sharma", "Sebastian Scherer", "Deva Ramanan"], "title": "Any4D: Unified Feed-Forward Metric 4D Reconstruction", "comment": "Project Website: https://any-4d.github.io/", "summary": "We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.", "AI": {"tldr": "Any4D\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u89c6\u89d2Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u5ea6\u91cf\u5c3a\u5ea6\u3001\u5bc6\u96c6\u524d\u99884D\u91cd\u5efa\uff0c\u80fd\u591f\u76f4\u63a5\u751f\u6210\u591a\u5e27\u56fe\u50cf\u7684\u50cf\u7d20\u7ea7\u8fd0\u52a8\u548c\u51e0\u4f55\u9884\u6d4b\uff0c\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u6a21\u6001\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce82\u89c6\u89d2\u5bc6\u96c6\u573a\u666f\u6d41\u6216\u7a00\u758f3D\u70b9\u8ddf\u8e2a\uff0c\u800cAny4D\u65e8\u5728\u89e3\u51b3\u66f4\u5168\u9762\u76844D\u91cd\u5efa\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u8f93\u5165\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u548c\u5b9e\u7528\u76844D\u573a\u666f\u91cd\u5efa\u6846\u67b6\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u76844D\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff1a\u5c06\u6bcf\u89c6\u89d2\u76844D\u9884\u6d4b\u7f16\u7801\u4e3a\u4ee5\u5c40\u90e8\u76f8\u673a\u5750\u6807\u7cfb\u8868\u793a\u7684\u81ea\u6211\u4e2d\u5fc3\u56e0\u7d20\uff08\u6df1\u5ea6\u56fe\u548c\u76f8\u673a\u5185\u53c2\uff09\u548c\u4ee5\u5168\u5c40\u4e16\u754c\u5750\u6807\u7cfb\u8868\u793a\u7684\u4ed6\u8005\u4e2d\u5fc3\u56e0\u7d20\uff08\u76f8\u673a\u5916\u53c2\u548c\u573a\u666f\u6d41\uff09\u3002\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u591a\u89c6\u89d2Transformer\u67b6\u6784\u5904\u7406\u591a\u5e27\u8f93\u5165\u3002", "result": "\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\uff1a\u7cbe\u5ea6\u65b9\u9762\u8bef\u5dee\u964d\u4f4e2-3\u500d\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u9ad815\u500d\uff0c\u4e3a\u591a\u4e2a\u4e0b\u6e38\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "Any4D\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u9ad8\u6548\u76844D\u91cd\u5efa\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u4f20\u611f\u5668\u6a21\u6001\uff0c\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.10940", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10940", "abs": "https://arxiv.org/abs/2512.10940", "authors": ["Xiang Fan", "Sharath Girish", "Vivek Ramanujan", "Chaoyang Wang", "Ashkan Mirzaei", "Petr Sushko", "Aliaksandr Siarohin", "Sergey Tulyakov", "Ranjay Krishna"], "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis", "comment": "Project page: https://snap-research.github.io/OmniView/", "summary": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/", "AI": {"tldr": "OmniView\u662f\u4e00\u4e2a\u7edf\u4e00\u76844D\u4e00\u81f4\u6027\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u76f8\u673a\u63a7\u5236\u4efb\u52a1\uff0c\u5305\u62ec\u65b0\u9896\u89c6\u56fe\u5408\u6210\u3001\u6587\u672c/\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u7b49\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u76f8\u673a\u63a7\u5236\u6ce8\u5165\u6269\u6563\u6a21\u578b\u65f6\uff0c\u53ea\u5173\u6ce8\u7279\u5b9a\u76844D\u4e00\u81f4\u6027\u4efb\u52a1\u5b50\u96c6\uff08\u5982\u65b0\u9896\u89c6\u56fe\u5408\u6210\u3001\u6587\u672c\u5230\u89c6\u9891\u7b49\uff09\uff0c\u8fd9\u4e9b\u788e\u7247\u5316\u65b9\u6cd5\u5728\u4e0d\u540c\u7c7b\u578b\u76843D/4D\u6570\u636e\u4e0a\u5206\u522b\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6027\u3002", "method": "OmniView\u91c7\u7528\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u89c6\u56fe\u6761\u4ef6\u5206\u522b\u8868\u793a\uff0c\u5141\u8bb8\u8fd9\u4e9b\u8f93\u5165\u7684\u7075\u6d3b\u7ec4\u5408\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u9759\u6001\u3001\u52a8\u6001\u548c\u591a\u89c6\u56fe\u8f93\u5165\u5408\u6210\u65b0\u9896\u89c6\u56fe\uff0c\u5728\u65f6\u95f4\u4e0a\u524d\u5411\u548c\u540e\u5411\u5916\u63a8\u8f68\u8ff9\uff0c\u4ee5\u53ca\u4ece\u6587\u672c\u6216\u56fe\u50cf\u63d0\u793a\u521b\u5efa\u5177\u6709\u5b8c\u6574\u76f8\u673a\u63a7\u5236\u7684\u89c6\u9891\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOmniView\u4e0e\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff1a\u5728\u591a\u89c6\u56feNVS LLFF\u6570\u636e\u96c6\u4e0a\u56fe\u50cf\u8d28\u91cf\u5f97\u5206\u63d0\u9ad833%\uff0c\u5728\u52a8\u6001NVS Neural 3D Video\u57fa\u51c6\u4e0a\u63d0\u9ad860%\uff0c\u5728RE-10K\u9759\u6001\u76f8\u673a\u63a7\u5236\u4e0a\u63d0\u9ad820%\uff0c\u5728\u6587\u672c\u6761\u4ef6\u89c6\u9891\u751f\u6210\u4e2d\u76f8\u673a\u8f68\u8ff9\u8bef\u5dee\u51cf\u5c114\u500d\u3002", "conclusion": "OmniView\u901a\u8fc7\u4e00\u4e2a\u6a21\u578b\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u901a\u75284D\u89c6\u9891\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u7edf\u4e00\u76844D\u4e00\u81f4\u6027\u4efb\u52a1\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10941", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10941", "abs": "https://arxiv.org/abs/2512.10941", "authors": ["Arijit Ray", "Ahmed Abdelkader", "Chengzhi Mao", "Bryan A. Plummer", "Kate Saenko", "Ranjay Krishna", "Leonidas Guibas", "Wen-Sheng Chu"], "title": "Mull-Tokens: Modality-Agnostic Latent Thinking", "comment": "Project webpage: https://arijitray.com/multimodal_thinking/", "summary": "Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.", "AI": {"tldr": "Mull-Tokens\u662f\u4e00\u79cd\u6a21\u6001\u65e0\u5173\u7684\u6f5c\u5728\u4ee4\u724c\uff0c\u5141\u8bb8\u6a21\u578b\u5728\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\u4e4b\u95f4\u81ea\u7531\u601d\u8003\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u7406\u95ee\u9898\uff0c\u5728\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u53473%\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u5728\u7a7a\u95f4\u3001\u65f6\u95f4\u3001\u53ef\u7528\u6027\u7b49\u771f\u5b9e\u4e16\u754c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u4f9d\u8d56\u4e8e\u8c03\u7528\u4e13\u4e1a\u5de5\u5177\u3001\u6602\u8d35\u7684\u56fe\u50cf\u751f\u6210\u6216\u624b\u5de5\u5236\u4f5c\u7684\u6570\u636e\u5728\u6587\u672c\u548c\u56fe\u50cf\u601d\u7ef4\u4e4b\u95f4\u5207\u6362\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8106\u5f31\u4e14\u4e0d\u53ef\u6269\u5c55\u3002", "method": "\u63d0\u51faMull-Tokens\uff08\u6a21\u6001\u65e0\u5173\u6f5c\u5728\u4ee4\u724c\uff09\uff0c\u9884\u8bad\u7ec3\u8fd9\u4e9b\u4ee4\u724c\u4ee5\u5728\u56fe\u50cf\u6216\u6587\u672c\u6a21\u6001\u4e2d\u4fdd\u5b58\u4e2d\u95f4\u4fe1\u606f\uff0c\u8ba9\u6a21\u578b\u81ea\u7531\u601d\u8003\u4ee5\u5f97\u51fa\u6b63\u786e\u7b54\u6848\u3002\u9996\u5148\u4f7f\u7528\u4ea4\u9519\u6587\u672c-\u56fe\u50cf\u8f68\u8ff9\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\uff0c\u7136\u540e\u4ec5\u4f7f\u7528\u6700\u7ec8\u7b54\u6848\u8fdb\u884c\u65e0\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5305\u62ec\u89e3\u8c1c\u548c\u4e0d\u540c\u89c6\u89d2\u4efb\u52a1\uff09\uff0cMull-Tokens\u76f8\u6bd4\u4ec5\u4f7f\u7528\u6587\u672c\u63a8\u7406\u6216\u4ea4\u9519\u56fe\u50cf-\u6587\u672c\u63a8\u7406\u7684\u57fa\u7ebf\u65b9\u6cd5\u6709\u6240\u6539\u8fdb\uff0c\u5e73\u5747\u63d0\u53473%\uff0c\u5728\u63a8\u7406\u5bc6\u96c6\u7684\u8c1c\u9898\u89e3\u51b3\u4efb\u52a1\u4e2d\u6700\u9ad8\u63d0\u534716%\u3002", "conclusion": "Mull-Tokens\u4e3a\u6587\u672c\u548c\u89c6\u89c9\u63a8\u7406\u7684\u843d\u5730\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u89e3\u51b3\u65b9\u6848\uff0c\u5141\u8bb8\u5728\u591a\u4e2a\u6a21\u6001\u4e2d\u8fdb\u884c\u62bd\u8c61\u601d\u8003\uff0c\u8d85\u8d8a\u4e86\u7eaf\u8bed\u8a00\u63a8\u7406\u7684\u9650\u5236\u3002"}}
{"id": "2512.10943", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10943", "abs": "https://arxiv.org/abs/2512.10943", "authors": ["Sharath Girish", "Viacheslav Ivanov", "Tsai-Shien Chen", "Hao Chen", "Aliaksandr Siarohin", "Sergey Tulyakov"], "title": "AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation", "comment": "Project page: https://snap-research.github.io/Video-AlcheMinT/snap-research.github.io/Video-AlcheMinT", "summary": "Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT", "AI": {"tldr": "AlcheMinT\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u65f6\u95f4\u6233\u6761\u4ef6\u63a7\u5236\uff0c\u5b9e\u73b0\u4e3b\u9898\u9a71\u52a8\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u63a7\u5236\uff0c\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u591a\u4e2a\u4e3b\u9898\u5728\u89c6\u9891\u4e2d\u7684\u51fa\u73b0\u548c\u6d88\u5931\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u4e3b\u9898\u9a71\u52a8\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u4e3b\u9898\u51fa\u73b0\u548c\u6d88\u5931\u7684\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u63a7\u5236\uff0c\u8fd9\u5728\u5408\u6210\u89c6\u9891\u3001\u6545\u4e8b\u677f\u548c\u53ef\u63a7\u52a8\u753b\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u89e3\u9501\u65f6\u95f4\u95f4\u9694\u7f16\u7801\uff08\u4e0e\u4e3b\u9898\u8eab\u4efd\u5173\u8054\uff09\uff0c\u5e76\u4e0e\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u4f4d\u7f6e\u5d4c\u5165\u65e0\u7f1d\u96c6\u6210\uff1b\u5f15\u5165\u4e3b\u9898\u63cf\u8ff0\u6027\u6587\u672c\u6807\u8bb0\u4ee5\u589e\u5f3a\u89c6\u89c9\u8eab\u4efd\u4e0e\u89c6\u9891\u63cf\u8ff0\u4e4b\u95f4\u7684\u7ed1\u5b9a\uff1b\u901a\u8fc7\u6807\u8bb0\u7ea7\u8fde\u63a5\u907f\u514d\u989d\u5916\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u53c2\u6570\u5f00\u9500\u6781\u5c0f\u3002", "result": "\u5efa\u7acb\u4e86\u8bc4\u4f30\u591a\u4e3b\u9898\u8eab\u4efd\u4fdd\u6301\u3001\u89c6\u9891\u4fdd\u771f\u5ea6\u548c\u65f6\u5e8f\u9075\u5faa\u7684\u57fa\u51c6\uff1b\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eAlcheMinT\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u89c6\u9891\u4e2a\u6027\u5316\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u9996\u6b21\u5b9e\u73b0\u4e86\u89c6\u9891\u5185\u591a\u4e3b\u9898\u751f\u6210\u7684\u7cbe\u786e\u65f6\u5e8f\u63a7\u5236\u3002", "conclusion": "AlcheMinT\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u65f6\u95f4\u6233\u6761\u4ef6\uff0c\u89e3\u51b3\u4e86\u4e3b\u9898\u9a71\u52a8\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u5408\u6210\u89c6\u9891\u3001\u6545\u4e8b\u677f\u548c\u53ef\u63a7\u52a8\u753b\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2512.10957", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10957", "abs": "https://arxiv.org/abs/2512.10957", "authors": ["Yukai Shi", "Weiyu Li", "Zihao Wang", "Hongyang Li", "Xingyu Chen", "Ping Tan", "Lei Zhang"], "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model", "comment": "Project page: https://idea-research.github.io/SceneMaker/", "summary": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.", "AI": {"tldr": "SceneMaker\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u76843D\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u53bb\u906e\u6321\u6a21\u578b\u548c3D\u5bf9\u8c61\u751f\u6210\uff0c\u5e76\u6539\u8fdb\u59ff\u6001\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4e25\u91cd\u906e\u6321\u548c\u5f00\u653e\u96c6\u573a\u666f\u4e0b\u7684\u51e0\u4f55\u8d28\u91cf\u548c\u59ff\u6001\u51c6\u786e\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e25\u91cd\u906e\u6321\u548c\u5f00\u653e\u96c6\u573a\u666f\u4e0b\uff0c\u7531\u4e8e\u7f3a\u4e4f\u8db3\u591f\u7684\u5f00\u653e\u96c6\u53bb\u906e\u6321\u548c\u59ff\u6001\u4f30\u8ba1\u5148\u9a8c\uff0c\u96be\u4ee5\u540c\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u548c\u51c6\u786e\u7684\u59ff\u6001\u3002", "method": "1. \u5c06\u53bb\u906e\u6321\u6a21\u578b\u4e0e3D\u5bf9\u8c61\u751f\u6210\u89e3\u8026\uff0c\u5229\u7528\u56fe\u50cf\u6570\u636e\u96c6\u548c\u6536\u96c6\u7684\u53bb\u906e\u6321\u6570\u636e\u96c6\u589e\u5f3a\u5f00\u653e\u96c6\u906e\u6321\u6a21\u5f0f\u591a\u6837\u6027\uff1b2. \u63d0\u51fa\u7edf\u4e00\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\uff0c\u6574\u5408\u5168\u5c40\u548c\u5c40\u90e8\u673a\u5236\u7684\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\uff1b3. \u6784\u5efa\u5f00\u653e\u96c63D\u573a\u666f\u6570\u636e\u96c6\u4ee5\u6269\u5c55\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u89e3\u8026\u6846\u67b6\u5728\u5ba4\u5185\u548c\u5f00\u653e\u96c6\u573a\u666f\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "SceneMaker\u901a\u8fc7\u89e3\u8026\u53bb\u906e\u6321\u548c3D\u751f\u6210\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u548c\u5f00\u653e\u96c6\u6570\u636e\u96c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u906e\u6321\u573a\u666f\u4e0b\u76843D\u573a\u666f\u751f\u6210\u95ee\u9898\u3002"}}
